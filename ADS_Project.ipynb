{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85764148",
   "metadata": {},
   "source": [
    "****\n",
    "<div class='heading'>\n",
    "    <div style='float:left;'><h1>CPSC 6300: Applied Data Science Final Project</h1></div>\n",
    "    <img style=\"float: right; padding-right: 5px; width: 80px\" src=\"https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/images/clemson_paw.png\"> </div>\n",
    "Clemson University<br>\n",
    "Spring 2023<br>\n",
    "\n",
    "## TOPIC - Continuous Affect Recognition from Multimodal Signals in Videos\n",
    "\n",
    "**Instructor(s):** Nina Hubig <br>\n",
    "\n",
    "**Group Members:** Charanjit Singh (C15246652) | Parampreet Singh (C19377466) | Vinod Ramavath (C13139775)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a493141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "\n",
       "div.heading {\n",
       "margin-bottom: 25px;\n",
       "height: 75px;\n",
       "}\n",
       "\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "    \n",
       "    background: rgba(245, 102, 0, .75);\n",
       "    border-color: #E9967A;\n",
       "    border-left: 5px solid #522D80; \n",
       "    padding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "    background-color: #fce8e8;\n",
       "    border-color: #E9967A; \t\n",
       "    border-left: 5px solid #800080; \n",
       "    padding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "    font-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "    background-color: #DDDDDD;\n",
       "    border-color: #E9967A; \t\n",
       "    border-left: 5px solid #800080; \n",
       "    padding: 0.5em;\n",
       "    font-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "    background-color: #AEDE94;\n",
       "    border-color: #E9967A; \t \n",
       "    border-left: 5px solid #800080; \n",
       "    padding: 0.5em;\n",
       "    font-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" RUN THIS CELL TO GET THE RIGHT FORMATTING \"\"\"\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "css_file = 'https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/css/cpsc6300.css'\n",
    "styles = requests.get(css_file).text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066697a0",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8403ae21",
   "metadata": {},
   "source": [
    "### Visual Representation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4405650",
   "metadata": {},
   "source": [
    "#### 1. Extracting Frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e43df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "# Set the paths to the input and output directories\n",
    "input_dir = \"/Data/OMG_Emotion_videos/videos\"\n",
    "output_dir = \"/Data/Frames\"\n",
    "\n",
    "k = 1\n",
    "# Loop through all of the video files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    print(k)\n",
    "    vid_path = os.path.join(input_dir, filename, \"video\")\n",
    "    for vid_name in os.listdir(vid_path):\n",
    "        if vid_name.endswith(\".mp4\"):\n",
    "            # Set the path to the input video file and create a new directory for the output frames\n",
    "            input_path = os.path.join(vid_path, vid_name)\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "            # Open the video file using OpenCV\n",
    "            cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "\n",
    "            count = 0\n",
    "            # Loop through all of the frames in the video\n",
    "            while True:\n",
    "                # Read the next frame from the video\n",
    "                while count < 10:\n",
    "                    count += 1\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                if not ret:\n",
    "                        break  \n",
    "    \n",
    "                # Save the frame as an image file in the output directory\n",
    "                output_file = os.path.join(output_path, f\"{vid_name}{count}.jpg\")\n",
    "                cv2.imwrite(output_file, frame)\n",
    "                # Increment the frame counter\n",
    "                count = 0\n",
    "\n",
    "            # Release the video capture object\n",
    "            cap.release()\n",
    "    k += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc254777",
   "metadata": {},
   "source": [
    "#### 2. Applying MTCNN to Align Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from mtcnn import MTCNN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load MTCNN model\n",
    "mtcnn_model = MTCNN()\n",
    "\n",
    "# Set the paths to the input and output directories\n",
    "input_dir = \"/Data/Frames\"\n",
    "output_dir = \"/Data/DataLoader/Aligned_faces\"\n",
    "\n",
    "# Loop through all of the video files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    print(k)\n",
    "    img_path = os.path.join(input_dir, filename)\n",
    "    for img_name in os.listdir(img_path):\n",
    "        if img_name.endswith(\".jpg\"):\n",
    "            # Set the path to the input video file and create a new directory for the output frames\n",
    "            input_path = os.path.join(img_path, img_name)\n",
    "            output_path = os.path.join(output_dir, filename, img_name)\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "            # load image\n",
    "            img = cv2.imread(input_path)\n",
    "\n",
    "            # detect face using MTCNN\n",
    "            boxes= mtcnn_model.detect_faces(img)\n",
    "\n",
    "            if boxes is not None:\n",
    "                # extract the first detected face\n",
    "                box = boxes[0]['box']\n",
    "                x, y, w, h = box\n",
    "\n",
    "                sliced_data = img[y:y+h, x:x+w]\n",
    "\n",
    "                # save the sliced image\n",
    "                plt.imsave(output_path, sliced_data)\n",
    "            else:\n",
    "                print('No face detected in the image.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25cf5e34",
   "metadata": {},
   "source": [
    "### Acoustic Representation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2510eae",
   "metadata": {},
   "source": [
    "#### Converted video snippets to WAV files and then extracted STFT maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f651f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import moviepy.editor as mp\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Set the paths to the input and output directories\n",
    "input_dir = \"/Data/OMG_Emotion_videos/videos\"\n",
    "output_dir = \"/Data/DataLoader/STFT_maps\"\n",
    "\n",
    "# Loop through all of the video files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    vid_path = os.path.join(input_dir, filename, \"video\")\n",
    "    for vid_name in os.listdir(vid_path):\n",
    "        if vid_name.endswith(\".mp4\"):\n",
    "            # Set the path to the input video file and create a new directory for the output frames\n",
    "            input_path = os.path.join(vid_path, vid_name)\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "            # Set sample rate, window size and hop length for STFT\n",
    "            sr = 16000\n",
    "            n_fft = 512\n",
    "            win_length = int(sr * 0.025)\n",
    "            hop_length = int(sr * 0.01)\n",
    "\n",
    "            # Extract audio from video and save to WAV files\n",
    "            audio = mp.AudioFileClip(input_path)\n",
    "            audio.write_audiofile(os.path.join(output_path, 'audio.wav'))\n",
    "\n",
    "            # Load audio from WAV file\n",
    "            audio, sr = librosa.load(os.path.join(output_path, 'audio.wav'), sr=sr, mono=True)\n",
    "\n",
    "            # Calculate STFT spectrograms\n",
    "            spec = librosa.stft(audio, n_fft=n_fft, win_length=win_length, hop_length=hop_length, window='hamming')\n",
    "\n",
    "            # Stack the real and imaginary parts of the STFT spectrograms\n",
    "            spec = np.stack([spec.real, spec.imag], axis=-1)\n",
    "\n",
    "            # Split the spectrograms into 3-second chunks\n",
    "            num_chunks = int(np.ceil(spec.shape[1] / (sr * 3)))\n",
    "            for i in range(num_chunks):\n",
    "                start_idx = i * sr * 3\n",
    "                end_idx = min((i + 1) * sr * 3, spec.shape[1])\n",
    "                chunk_spec = spec[:, start_idx:end_idx, :]\n",
    "\n",
    "                # Save each chunk of STFT spectrograms to a file\n",
    "                output_file = os.path.join(output_path, f'{vid_name}_spec_{i}.npy')\n",
    "                np.save(output_file, chunk_spec)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "214edca1",
   "metadata": {},
   "source": [
    "## MODEL IMPLEMENTATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bcb16f1",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "324fd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#import net_sphere\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import datetime,sys\n",
    "from numpy.random import randint\n",
    "import torchvision.models as models\n",
    "from calculateEvaluationCCC import calculateCCC\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define parameters\n",
    "use_cuda = torch.cuda.is_available()\n",
    "lr = 0.001\n",
    "bs = 6\n",
    "n_epoch = 30\n",
    "lr_steps = [7,14,21,28]\n",
    "gpu_id = [0,1]\n",
    "\n",
    "gd = 20 # clip gradient\n",
    "eval_freq = 3\n",
    "print_freq = 100\n",
    "num_worker = 4\n",
    "num_seg = 16\n",
    "num_stft = 4\n",
    "flag_biLSTM = True\n",
    "\n",
    "\n",
    "train_list_path = \"/Data/DataLoader/Annotations_train.csv\"\n",
    "val_list_path = \"/Data/DataLoader/Annotations_val.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "205b12ec",
   "metadata": {},
   "source": [
    "### VideoNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sphereface = getattr(net_sphere,'sphere20a')()\n",
    "sphereface.feature = True # remove the last fc layer because we need to use LSTM first\n",
    "\n",
    "class VNet(torch.nn.Module):\n",
    "    def __init__(self, sphereface, feature=True):\n",
    "        super(VNet, self).__init__()\n",
    "        self.sphereface = sphereface\n",
    "        self.linear = torch.nn.Linear(512,2)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.feature = feature\n",
    "        self.avgPool = torch.nn.AvgPool2d((num_seg,1), stride=1)\n",
    "        self.LSTM = torch.nn.LSTM(512, 512, 1, batch_first = True, dropout=0.2, bidirectional=flag_biLSTM)  # Input dim, hidden dim, num_layer\n",
    "        for name, param in self.LSTM.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                torch.nn.init.constant(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                torch.nn.init.orthogonal(param)\n",
    "        \n",
    "    def sequentialLSTM(self, input, hidden=None):\n",
    "\n",
    "        input_lstm = input.view([-1,num_seg, input.shape[1]])\n",
    "        batch_size = input_lstm.shape[0]\n",
    "        feature_size = input_lstm.shape[2]\n",
    "\n",
    "        self.LSTM.flatten_parameters()\n",
    "            \n",
    "        output_lstm, hidden = self.LSTM(input_lstm)\n",
    "        if flag_biLSTM:\n",
    "             output_lstm = output_lstm.contiguous().view(batch_size, output_lstm.size(1), 2, -1).sum(2).view(batch_size, output_lstm.size(1), -1) \n",
    "\n",
    "        output_lstm = output_lstm.view(batch_size,1,num_seg,-1)\n",
    "        out = self.avgPool(output_lstm)\n",
    "        out = out.view(batch_size,-1)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sphereface(x)\n",
    "        x = self.sequentialLSTM(x)\n",
    "        if self.feature == True: return x\n",
    "\n",
    "        x = self.linear(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model_v = VNet(sphereface)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f5036a1",
   "metadata": {},
   "source": [
    "### AudioNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d5f2473",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg16(pretrained=False).features\n",
    "\n",
    "removed = list(vgg.children())[1:]\n",
    "vgg = torch.nn.Sequential(*removed)\n",
    "\n",
    "# We modified the first layer of vgg16\n",
    "vgg_modified = torch.nn.Sequential(torch.nn.Conv2d(2,64,3),vgg)\n",
    "\n",
    "class ANet(torch.nn.Module):\n",
    "    def __init__(self, vgg,feature=True):\n",
    "        super(ANet, self).__init__()\n",
    "        self.vgg = vgg\n",
    "        self.fc1 = torch.nn.Linear(512*7*9,4096)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        self.fc2 = torch.nn.Linear(4096,512)\n",
    "        self.fc3 = torch.nn.Linear(512,2)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.feature = feature\n",
    "    def forward(self, x):\n",
    "        x = self.vgg(x)\n",
    "        x = x.view([-1,512*7*9])\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        if self.feature == True: return x \n",
    "        \n",
    "        x = self.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "model_a = ANet(vgg_modified)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c900213",
   "metadata": {},
   "source": [
    "### Joint Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ee64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AVNet(torch.nn.Module):\n",
    "    def __init__(self, vnet,anet):\n",
    "        super(AVNet, self).__init__()\n",
    "        self.vnet = vnet\n",
    "        self.anet = anet\n",
    "        self.avgPool = torch.nn.AvgPool2d((num_stft,1), stride=1)\n",
    "        self.fc = torch.nn.Linear(1024,2)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, xi,xs):\n",
    "        xi = self.vnet(xi) \n",
    "        xs = self.anet(xs)\n",
    "        xs = xs.view((-1,1,num_stft,512))\n",
    "        xs = self.avgPool(xs)\n",
    "        xs = xs.view(-1,512)\n",
    "        \n",
    "        \n",
    "        x = torch.cat((xi, xs), 1)\n",
    "        x = self.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "xi = torch.autograd.Variable(torch.randn(32,3,96,112))\n",
    "xs = torch.autograd.Variable(torch.randn(8,2,257,300))\n",
    "\n",
    "model = AVNet(model_v, model_a)\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e42745be",
   "metadata": {},
   "source": [
    "### OMG Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b555aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OMGDataset(Dataset):\n",
    "    \"\"\"OMG dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, txt_file, base_path_v, base_path_a, transform=None):\n",
    "        self.base_path_v = base_path_v\n",
    "        self.base_path_a = base_path_a\n",
    "        self.data = pd.read_csv(txt_file, sep=\" \", header=None)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid = self.data.iloc[idx,0]\n",
    "        utter = self.data.iloc[idx,1]\n",
    "        img_list = self.data.iloc[idx,-1]\n",
    "        img_list = img_list.split(',')[:-1]\n",
    "        img_list = map(int, img_list)\n",
    "        \n",
    "        num_frames = len(img_list)\n",
    "        # inspired by TSN's pytorch code\n",
    "        average_duration = num_frames // num_seg\n",
    "        if num_frames>num_seg:\n",
    "            offsets = np.multiply(list(range(num_seg)), average_duration) + randint(average_duration, size=num_seg)\n",
    "        else:\n",
    "            tick = num_frames / float(num_seg)\n",
    "            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(num_seg)])\n",
    "\n",
    "        final_list = [img_list[i] for i in offsets]\n",
    "        \n",
    "        # stack images within a video in the depth dimension\n",
    "        for i,ind in enumerate(final_list):\n",
    "            image = io.imread(self.base_path_v+'%s/%s/%d.jpg'%(vid,utter,ind)).astype(np.float32)\n",
    "            image = torch.from_numpy(((image - 127.5)/128).transpose(2,0,1))\n",
    "            if i==0:\n",
    "                images = image\n",
    "            else:\n",
    "                images = torch.cat((images,image), 0)\n",
    "\n",
    "        # stft data acquisition\n",
    "        stft_path = self.base_path_a+vid+'/'+utter\n",
    "        stfts_count = len(glob.glob1(stft_path,\"*.npy\"))\n",
    "        stft_list_all = range(stfts_count)\n",
    "        \n",
    "        average_duration = stfts_count // num_stft\n",
    "        if stfts_count>num_stft:\n",
    "            offsets = np.multiply(list(range(num_stft)), average_duration) + randint(average_duration, size=num_stft)\n",
    "        else:\n",
    "            tick = stfts_count / float(num_stft)\n",
    "            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(num_stft)])\n",
    "        \n",
    "        stft_list = [stft_list_all[i] for i in offsets]\n",
    "        \n",
    "        for i,ind in enumerate(stft_list):\n",
    "            \n",
    "            stft = np.load(stft_path+'/%d.npy'%ind).astype(np.float32)\n",
    "            max_val = max(np.abs(np.max(stft)),np.abs(np.min(stft)))\n",
    "            mean_val = np.mean(stft)\n",
    "            stft = torch.from_numpy(((stft - mean_val)/max_val).transpose(2,0,1))\n",
    "            if i==0:\n",
    "                stfts = stft\n",
    "            else:\n",
    "                stfts = torch.cat((stfts,stft), 0)\n",
    "        \n",
    "        label = torch.from_numpy(np.array([self.data.iloc[idx,2], self.data.iloc[idx,3]]).astype(np.float32))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return (images, stfts, label, (vid,utter))\n",
    "\n",
    " \n",
    "base_path_v = \"/Data/DataLoader/Aligned_faces\"\n",
    "base_path_a = \"/Data/DataLoader/STFT_maps\"\n",
    "\n",
    "train_loader = DataLoader(OMGDataset(train_list_path,base_path_v, base_path_a), \n",
    "                          batch_size=bs, shuffle=True, num_workers=num_worker)\n",
    "val_loader = DataLoader(OMGDataset(val_list_path,base_path_v, base_path_a), \n",
    "                         batch_size=bs, shuffle=False, num_workers=num_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0976ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printoneline(*argv):\n",
    "    s = ''\n",
    "    for arg in argv: s += str(arg) + ' '\n",
    "    s = s[:-1]\n",
    "    sys.stdout.write('\\r'+s)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def dt():\n",
    "    return datetime.datetime.now().strftime('%H:%M:%S')\n",
    "\n",
    "def save_model(model,filename):\n",
    "    state = model.state_dict()\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20e02b91",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b7b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearsonr(outputs, targets):\n",
    "    vx = outputs - torch.mean(outputs)\n",
    "    vy = targets - torch.mean(targets)\n",
    "    rho = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))  # use Pearson correlation\n",
    "    return rho\n",
    "\n",
    "def calCCC(out, tar, rho):\n",
    "    true_mean = torch.mean(tar)\n",
    "    true_variance = torch.var(tar)\n",
    "    pred_mean = torch.mean(out)\n",
    "    pred_variance = torch.var(out)\n",
    "    std_predictions = torch.std(out)\n",
    "    std_gt = torch.std(tar)\n",
    "    \n",
    "    ccc = 2 * rho * std_gt * std_predictions / (\n",
    "        std_predictions ** 2 + std_gt ** 2 +\n",
    "        (pred_mean - true_mean) ** 2)\n",
    "    \n",
    "    return ccc\n",
    "\n",
    "def calLoss(outputs, targets):\n",
    "    out_a = outputs[:,0]\n",
    "    out_v = outputs[:,1]\n",
    "    tar_a = targets[:,0]\n",
    "    tar_v = targets[:,1]\n",
    "    \n",
    "    rho_a = pearsonr(out_a, tar_a)\n",
    "    rho_v = pearsonr(out_v, tar_v)\n",
    "    \n",
    "    ccc_a = calCCC(out_a,tar_a,rho_a)\n",
    "    ccc_v = calCCC(out_v,tar_v,rho_v)\n",
    "    \n",
    "    ccc_all = -(ccc_a+ccc_v)\n",
    "    return ccc_all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f3f818c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2892ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_idx = 0\n",
    "    \n",
    "    for i, (inputs_v, inputs_a, targets, _) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_cuda:\n",
    "            inputs_v, inputs_a, targets = inputs_v.cuda(), inputs_a.cuda(), targets.cuda()\n",
    "\n",
    "        inputs_v = torch.autograd.Variable(inputs_v)\n",
    "        inputs_a = torch.autograd.Variable(inputs_a)\n",
    "        targets = torch.autograd.Variable(targets)\n",
    "        \n",
    "        inputs_v = inputs_v.view((-1,3)+inputs_v.size()[-2:])\n",
    "        inputs_a = inputs_a.view((-1,2)+inputs_a.size()[-2:])\n",
    "        \n",
    "        outputs = model(inputs_v, inputs_a)\n",
    "        \n",
    "        loss = calLoss(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #tsn uses clipping gradient\n",
    "        if gd is not None:\n",
    "            total_norm = clip_grad_norm(model.parameters(),gd)\n",
    "            if total_norm > gd:\n",
    "                print('clippling gradient: {} with coef {}'.format(total_norm, gd/total_norm))\n",
    "                \n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        if i % print_freq == 0:\n",
    "            printoneline(dt(),'Epoch=%d Loss=%.4f\\n'\n",
    "                % (epoch,train_loss/(batch_idx+1)))\n",
    "        batch_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcdb11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    err_arou = 0.0\n",
    "    err_vale = 0.0\n",
    "    \n",
    "    out_name = 'results/joint_ccc2_%d.csv'%epoch\n",
    "    txt_result = open(out_name, 'w')\n",
    "    txt_result.write('video,utterance,arousal,valence\\n')\n",
    "    for (inputs_v, inputs_a, targets,(vid, utter)) in val_loader:\n",
    "        if use_cuda:\n",
    "            inputs_v, inputs_a, targets = inputs_v.cuda(), inputs_a.cuda(), targets.cuda()\n",
    "        \n",
    "        inputs_v = torch.autograd.Variable(inputs_v)\n",
    "        inputs_a = torch.autograd.Variable(inputs_a)\n",
    "        targets = torch.autograd.Variable(targets)\n",
    "        \n",
    "        \n",
    "        inputs_v = inputs_v.view((-1,3)+inputs_v.size()[-2:])\n",
    "        inputs_a = inputs_a.view((-1,2)+inputs_a.size()[-2:])\n",
    "        \n",
    "        outputs = model(inputs_v, inputs_a)\n",
    "        \n",
    "        for i in range(len(vid)):\n",
    "            out = outputs\n",
    "            txt_result.write('%s,%s.mp4,%f,%f\\n'%(vid[i], utter[i],out[i][0],out[i][1]))\n",
    "    \n",
    "    txt_result.close()\n",
    "    \n",
    "    arouCCC, valeCCC = calculateCCC('/results/omg_ValidationVideos.csv',out_name)\n",
    "    return (arouCCC,valeCCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ace0a5",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd21ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)    \n",
    "best_arou_ccc, best_vale_ccc = validate(val_loader, model, 0)\n",
    "for epoch in range(n_epoch):\n",
    "    if epoch in lr_steps:\n",
    "        lr *= 0.1\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)    \n",
    "\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on validation set\n",
    "    if (epoch+1)%eval_freq == 0 or epoch == n_epoch-1:\n",
    "        arou_ccc, vale_ccc = validate(val_loader, model, epoch)\n",
    "        \n",
    "        if (arou_ccc+vale_ccc) > (best_arou_ccc + best_vale_ccc):\n",
    "            best_arou_ccc = arou_ccc\n",
    "            best_vale_ccc = vale_ccc\n",
    "save_model(model,'/pth/joint_ccc2_{}_{}_{}.pth'.format(epoch, round(arou_ccc,4), round(vale_ccc,4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
